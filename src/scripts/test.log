main {
  Reading tokenizer configs from /share/pi/nigam/users/migufuen/helm/src/helm/config/tokenizer_configs.yaml...
  Reading model deployments from /share/pi/nigam/users/migufuen/helm/src/helm/config/model_deployments.yaml...
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct of model deployment migufuen/Llama-3.2-1B-Instruct
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.1 of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.1
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.2 of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.2
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.3 of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.3
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-4bit of model deployment migufuen/Llama-3.2-1B-Instruct-4bit
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-8bit of model deployment migufuen/Llama-3.2-1B-Instruct-8bit
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit
  Read 28 run entries from 5_evaluate_pruned_quantized_models.conf
  28 entries produced 28 run specs
  run_specs {
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
  } [0.0s]
  Running in local mode with base path: prod_env
Looking in path: prod_env
  AutoTokenizer: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  AutoClient: file_storage_path = prod_env/cache
  AutoClient: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  AutoTokenizer: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  Found 1 account(s).
Looking in path: prod_env
  AnnotatorFactory: file_storage_path = prod_env/cache
  AnnotatorFactory: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  Running med_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw&confirm=t because benchmark_output/scenarios/med_qa/data already exists
      } [0.053s]
    } [0.9s]
    12723 instances, 10178 train instances, 1000/2545 eval instances
    DataPreprocessor.preprocess {
    } [0.001s]
    MultipleChoiceJointAdapter.adapt {
      11178 instances, choosing 5/10178 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        } [1m9.984s]
        Sample prompts {
          reference index = None, request_mode = None {
            The following are multiple choice questions (with answers) about medicine.
            
            Question: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?
            A. CT scan of the pelvis
            B. Reassurance
            C. Combined oral contraceptive pill
            D. Pelvic ultrasonography
            Answer: B
            
            Question: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?
            A. Fecal occult blood testing
            B. Flexible sigmoidoscopy
            C. Low-dose CT
            D. Colonoscopy
            Answer: D
            
            Question: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?
            A. Streptococcus sanguinis
            B. Enterococcus faecalis
            C. Neisseria gonorrhoeae
            D. Staphylococcus aureus
            Answer: D
            
            Question: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?
            A. Indomethacin
            B. Aspirin
            C. Celecoxib
            D. Carbamazepine
            Answer: B
            
            Question: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?
            A. Furosemide
            B. Amiodarone
            C. Digoxin
            D. Lisinopril
            Answer: D
            
            Question: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?
            A. Papillary muscle rupture
            B. Ventricular fibrillation
            C. Septal wall rupture
            D. Pulmonary embolism
            Answer:
          } [0.0s]
        } [0.0s]
      } [1m10.01s]
      1000 requests
    } [1m10.012s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Loading meta-llama/Meta-Llama-3-8B (kwargs={}) for HELM tokenizer meta/llama-3-8b with Hugging Face Transformers {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        } [0.377s]
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [39.645s]
        } [39.773s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.263s]
        } [1.263s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.197s]
        } [1.197s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.22s]
        } [1.22s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.172s]
        } [1.172s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.227s]
        } [1.228s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.247s]
        } [1.247s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.22s]
        } [1.22s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.217s]
        } [1.217s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.254s]
        } [1.254s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.248s]
        } [1.248s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.226s]
        } [1.226s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.226s]
        } [1.226s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m51.788s]
    } [5m51.788s]
  } [7m2.988s]
  Error when running med_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', embedding=False, prompt="The following are multiple choice questions (with answers) about medicine.\n\nQuestion: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?\nA. CT scan of the pelvis\nB. Reassurance\nC. Combined oral contraceptive pill\nD. Pelvic ultrasonography\nAnswer: B\n\nQuestion: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?\nA. Fecal occult blood testing\nB. Flexible sigmoidoscopy\nC. Low-dose CT\nD. Colonoscopy\nAnswer: D\n\nQuestion: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?\nA. Streptococcus sanguinis\nB. Enterococcus faecalis\nC. Neisseria gonorrhoeae\nD. Staphylococcus aureus\nAnswer: D\n\nQuestion: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?\nA. Indomethacin\nB. Aspirin\nC. Celecoxib\nD. Carbamazepine\nAnswer: B\n\nQuestion: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?\nA. Furosemide\nB. Amiodarone\nC. Digoxin\nD. Lisinopril\nAnswer: D\n\nQuestion: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?\nA. Papillary muscle rupture\nB. Ventricular fibrillation\nC. Septal wall rupture\nD. Pulmonary embolism\nAnswer:", temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw&confirm=t because benchmark_output/scenarios/med_qa/data already exists
      } [0.111s]
    } [0.601s]
    12723 instances, 10178 train instances, 1000/2545 eval instances
    DataPreprocessor.preprocess {
    } [0.001s]
    MultipleChoiceJointAdapter.adapt {
      11178 instances, choosing 5/10178 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [21.88s]
        Sample prompts {
          reference index = None, request_mode = None {
            The following are multiple choice questions (with answers) about medicine.
            
            Question: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?
            A. CT scan of the pelvis
            B. Reassurance
            C. Combined oral contraceptive pill
            D. Pelvic ultrasonography
            Answer: B
            
            Question: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?
            A. Fecal occult blood testing
            B. Flexible sigmoidoscopy
            C. Low-dose CT
            D. Colonoscopy
            Answer: D
            
            Question: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?
            A. Streptococcus sanguinis
            B. Enterococcus faecalis
            C. Neisseria gonorrhoeae
            D. Staphylococcus aureus
            Answer: D
            
            Question: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?
            A. Indomethacin
            B. Aspirin
            C. Celecoxib
            D. Carbamazepine
            Answer: B
            
            Question: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?
            A. Furosemide
            B. Amiodarone
            C. Digoxin
            D. Lisinopril
            Answer: D
            
            Question: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?
            A. Papillary muscle rupture
            B. Ventricular fibrillation
            C. Septal wall rupture
            D. Pulmonary embolism
            Answer:
          } [0.0s]
        } [0.0s]
      } [21.907s]
      1000 requests
    } [21.909s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.321s]
        } [1.321s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.178s]
        } [1.178s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.169s]
        } [1.169s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.158s]
        } [1.158s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.202s]
        } [1.202s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.205s]
        } [1.205s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.183s]
        } [1.183s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.183s]
        } [1.183s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.161s]
        } [1.162s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.153s]
        } [1.153s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.142s]
        } [1.142s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.191s]
        } [1.191s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.157s]
        } [1.157s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.142s]
        } [1.142s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.13s]
        } [1.13s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.177s]
        } [1.178s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.135s]
        } [1.135s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.152s]
        } [1.152s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.135s]
        } [1.135s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.185s]
        } [1.185s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.148s]
        } [1.148s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.141s]
        } [1.141s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.18s]
        } [1.18s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.172s]
        } [1.172s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.301s]
        } [1.301s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m12.214s]
    } [5m12.214s]
  } [5m34.965s]
  Error when running med_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', embedding=False, prompt="The following are multiple choice questions (with answers) about medicine.\n\nQuestion: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?\nA. CT scan of the pelvis\nB. Reassurance\nC. Combined oral contraceptive pill\nD. Pelvic ultrasonography\nAnswer: B\n\nQuestion: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?\nA. Fecal occult blood testing\nB. Flexible sigmoidoscopy\nC. Low-dose CT\nD. Colonoscopy\nAnswer: D\n\nQuestion: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?\nA. Streptococcus sanguinis\nB. Enterococcus faecalis\nC. Neisseria gonorrhoeae\nD. Staphylococcus aureus\nAnswer: D\n\nQuestion: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?\nA. Indomethacin\nB. Aspirin\nC. Celecoxib\nD. Carbamazepine\nAnswer: B\n\nQuestion: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?\nA. Furosemide\nB. Amiodarone\nC. Digoxin\nD. Lisinopril\nAnswer: D\n\nQuestion: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?\nA. Papillary muscle rupture\nB. Ventricular fibrillation\nC. Septal wall rupture\nD. Pulmonary embolism\nAnswer:", temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw&confirm=t because benchmark_output/scenarios/med_qa/data already exists
      } [0.062s]
    } [0.56s]
    12723 instances, 10178 train instances, 1000/2545 eval instances
    DataPreprocessor.preprocess {
    } [0.001s]
    MultipleChoiceJointAdapter.adapt {
      11178 instances, choosing 5/10178 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [23.287s]
        Sample prompts {
          reference index = None, request_mode = None {
            The following are multiple choice questions (with answers) about medicine.
            
            Question: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?
            A. CT scan of the pelvis
            B. Reassurance
            C. Combined oral contraceptive pill
            D. Pelvic ultrasonography
            Answer: B
            
            Question: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?
            A. Fecal occult blood testing
            B. Flexible sigmoidoscopy
            C. Low-dose CT
            D. Colonoscopy
            Answer: D
            
            Question: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?
            A. Streptococcus sanguinis
            B. Enterococcus faecalis
            C. Neisseria gonorrhoeae
            D. Staphylococcus aureus
            Answer: D
            
            Question: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?
            A. Indomethacin
            B. Aspirin
            C. Celecoxib
            D. Carbamazepine
            Answer: B
            
            Question: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?
            A. Furosemide
            B. Amiodarone
            C. Digoxin
            D. Lisinopril
            Answer: D
            
            Question: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?
            A. Papillary muscle rupture
            B. Ventricular fibrillation
            C. Septal wall rupture
            D. Pulmonary embolism
            Answer:
          } [0.0s]
        } [0.0s]
      } [23.315s]
      1000 requests
    } [23.317s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [9.084s]
        } [9.084s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.735s]
        } [1.735s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.569s]
        } [1.57s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.596s]
        } [1.596s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.608s]
        } [1.608s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.657s]
        } [1.657s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.572s]
        } [1.572s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.63s]
        } [1.63s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.587s]
        } [1.587s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.554s]
        } [1.554s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.545s]
        } [1.546s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.58s]
        } [1.581s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.626s]
        } [1.627s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.614s]
        } [1.614s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.549s]
        } [1.549s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.62s]
        } [1.62s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.593s]
        } [1.593s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.618s]
        } [1.618s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.56s]
        } [1.56s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.591s]
        } [1.592s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.582s]
        } [1.582s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.616s]
        } [1.616s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.622s]
        } [1.622s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.624s]
        } [1.624s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.654s]
        } [1.654s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m23.814s]
    } [5m23.814s]
  } [5m47.925s]
  Error when running med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', embedding=False, prompt="The following are multiple choice questions (with answers) about medicine.\n\nQuestion: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?\nA. CT scan of the pelvis\nB. Reassurance\nC. Combined oral contraceptive pill\nD. Pelvic ultrasonography\nAnswer: B\n\nQuestion: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?\nA. Fecal occult blood testing\nB. Flexible sigmoidoscopy\nC. Low-dose CT\nD. Colonoscopy\nAnswer: D\n\nQuestion: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?\nA. Streptococcus sanguinis\nB. Enterococcus faecalis\nC. Neisseria gonorrhoeae\nD. Staphylococcus aureus\nAnswer: D\n\nQuestion: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?\nA. Indomethacin\nB. Aspirin\nC. Celecoxib\nD. Carbamazepine\nAnswer: B\n\nQuestion: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?\nA. Furosemide\nB. Amiodarone\nC. Digoxin\nD. Lisinopril\nAnswer: D\n\nQuestion: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?\nA. Papillary muscle rupture\nB. Ventricular fibrillation\nC. Septal wall rupture\nD. Pulmonary embolism\nAnswer:", temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw&confirm=t because benchmark_output/scenarios/med_qa/data already exists
      } [0.486s]
    } [0.948s]
    12723 instances, 10178 train instances, 1000/2545 eval instances
    DataPreprocessor.preprocess {
    } [0.001s]
    MultipleChoiceJointAdapter.adapt {
      11178 instances, choosing 5/10178 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [29.571s]
        Sample prompts {
          reference index = None, request_mode = None {
            The following are multiple choice questions (with answers) about medicine.
            
            Question: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?
            A. CT scan of the pelvis
            B. Reassurance
            C. Combined oral contraceptive pill
            D. Pelvic ultrasonography
            Answer: B
            
            Question: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?
            A. Fecal occult blood testing
            B. Flexible sigmoidoscopy
            C. Low-dose CT
            D. Colonoscopy
            Answer: D
            
            Question: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?
            A. Streptococcus sanguinis
            B. Enterococcus faecalis
            C. Neisseria gonorrhoeae
            D. Staphylococcus aureus
            Answer: D
            
            Question: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?
            A. Indomethacin
            B. Aspirin
            C. Celecoxib
            D. Carbamazepine
            Answer: B
            
            Question: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?
            A. Furosemide
            B. Amiodarone
            C. Digoxin
            D. Lisinopril
            Answer: D
            
            Question: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?
            A. Papillary muscle rupture
            B. Ventricular fibrillation
            C. Septal wall rupture
            D. Pulmonary embolism
            Answer:
          } [0.0s]
        } [0.0s]
      } [29.6s]
      1000 requests
    } [29.602s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.784s]
        } [1.784s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.6s]
        } [1.6s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
          Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.586s]
        } [1.586s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.589s]
        } [1.589s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.644s]
        } [1.644s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.611s]
        } [1.611s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.589s]
        } [1.589s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.597s]
        } [1.597s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.616s]
        } [1.616s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.595s]
        } [1.595s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.575s]
        } [1.575s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.58s]
        } [1.58s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.585s]
        } [1.585s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.574s]
        } [1.574s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.565s]
        } [1.565s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.626s]
        } [1.626s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.585s]
        } [1.585s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.574s]
        } [1.574s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.601s]
        } [1.601s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.561s]
        } [1.561s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.635s]
        } [1.635s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.597s]
        } [1.597s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.603s]
        } [1.603s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.585s]
        } [1.585s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [1.587s]
        } [1.587s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m18.042s]
    } [5m18.042s]
  } [5m48.787s]
  Error when running med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', embedding=False, prompt="The following are multiple choice questions (with answers) about medicine.\n\nQuestion: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?\nA. CT scan of the pelvis\nB. Reassurance\nC. Combined oral contraceptive pill\nD. Pelvic ultrasonography\nAnswer: B\n\nQuestion: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?\nA. Fecal occult blood testing\nB. Flexible sigmoidoscopy\nC. Low-dose CT\nD. Colonoscopy\nAnswer: D\n\nQuestion: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?\nA. Streptococcus sanguinis\nB. Enterococcus faecalis\nC. Neisseria gonorrhoeae\nD. Staphylococcus aureus\nAnswer: D\n\nQuestion: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?\nA. Indomethacin\nB. Aspirin\nC. Celecoxib\nD. Carbamazepine\nAnswer: B\n\nQuestion: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?\nA. Furosemide\nB. Amiodarone\nC. Digoxin\nD. Lisinopril\nAnswer: D\n\nQuestion: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?\nA. Papillary muscle rupture\nB. Ventricular fibrillation\nC. Septal wall rupture\nD. Pulmonary embolism\nAnswer:", temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-4bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=15VkJdq5eyWIkfb_aoD3oS8i4tScbHYky&confirm=t because benchmark_output/scenarios/med_mcqa/data already exists
      } [0.117s]
    } [6.13s]
    187005 instances, 182822 train instances, 1000/4183 eval instances
    DataPreprocessor.preprocess {
    } [0.028s]
    MultipleChoiceJointAdapter.adapt {
      183822 instances, choosing 5/182822 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [24.281s]
        Sample prompts {
          reference index = None, request_mode = None {
            Give a letter answer among A, B, C or D.
            
            Question: Increased perinatal moality in diabetic pregnancy is due to:
            A. Congenital malformations
            B. Hypoglycemia
            C. Hyaline membrane disease
            D. All of the above
            Answer: D
            
            Question: Necrotic Keratinocyts occur in
            A. DLE
            B. Graft versus host disease
            C. Erythema multiformal
            D. All
            Answer: D
            
            Question: For systemic mycosis fluconazole is preferred over ketoconazole because of -
            A. Greater efficacy
            B. Longer t1/2
            C. Lesser side effects
            D. All the above
            Answer: D
            
            Question: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction
            A. ab
            B. a
            C. ad
            D. bc
            Answer: A
            
            Question: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -
            A. Additional intake of 300 K.cal
            B. Additional intake of 500 K.cal
            C. Additional intake of 650 K.cal
            D. None
            Answer: D
            
            Question: Highest level of evidence is seen in:
            A. Case control studies
            B. Meta-analysis
            C. Cohort studies
            D. Systematic review
            Answer:
          } [0.0s]
        } [0.0s]
      } [24.784s]
      1000 requests
    } [24.811s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [6.958s]
        } [6.958s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.282s]
        } [2.282s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.21s]
        } [2.21s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.196s]
        } [2.196s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.154s]
        } [2.154s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.17s]
        } [2.171s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.167s]
        } [2.167s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.186s]
        } [2.186s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.144s]
        } [2.144s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.17s]
        } [2.17s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.156s]
        } [2.156s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.23s]
        } [2.23s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.236s]
        } [2.236s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.23s]
        } [2.23s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.206s]
        } [2.206s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.179s]
        } [2.179s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.217s]
        } [2.218s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.229s]
        } [2.229s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.209s]
        } [2.209s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.192s]
        } [2.192s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.204s]
        } [2.204s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.225s]
        } [2.225s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.219s]
        } [2.219s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.22s]
        } [2.22s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.25s]
        } [2.25s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m27.03s]
    } [5m27.031s]
  } [5m59.665s]
  Error when running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-4bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', embedding=False, prompt='Give a letter answer among A, B, C or D.\n\nQuestion: Increased perinatal moality in diabetic pregnancy is due to:\nA. Congenital malformations\nB. Hypoglycemia\nC. Hyaline membrane disease\nD. All of the above\nAnswer: D\n\nQuestion: Necrotic Keratinocyts occur in\nA. DLE\nB. Graft versus host disease\nC. Erythema multiformal\nD. All\nAnswer: D\n\nQuestion: For systemic mycosis fluconazole is preferred over ketoconazole because of -\nA. Greater efficacy\nB. Longer t1/2\nC. Lesser side effects\nD. All the above\nAnswer: D\n\nQuestion: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction\nA. ab\nB. a\nC. ad\nD. bc\nAnswer: A\n\nQuestion: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -\nA. Additional intake of 300 K.cal\nB. Additional intake of 500 K.cal\nC. Additional intake of 650 K.cal\nD. None\nAnswer: D\n\nQuestion: Highest level of evidence is seen in:\nA. Case control studies\nB. Meta-analysis\nC. Cohort studies\nD. Systematic review\nAnswer:', temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-8bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=15VkJdq5eyWIkfb_aoD3oS8i4tScbHYky&confirm=t because benchmark_output/scenarios/med_mcqa/data already exists
      } [0.042s]
    } [5.077s]
    187005 instances, 182822 train instances, 1000/4183 eval instances
    DataPreprocessor.preprocess {
    } [0.027s]
    MultipleChoiceJointAdapter.adapt {
      183822 instances, choosing 5/182822 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [22.435s]
        Sample prompts {
          reference index = None, request_mode = None {
            Give a letter answer among A, B, C or D.
            
            Question: Increased perinatal moality in diabetic pregnancy is due to:
            A. Congenital malformations
            B. Hypoglycemia
            C. Hyaline membrane disease
            D. All of the above
            Answer: D
            
            Question: Necrotic Keratinocyts occur in
            A. DLE
            B. Graft versus host disease
            C. Erythema multiformal
            D. All
            Answer: D
            
            Question: For systemic mycosis fluconazole is preferred over ketoconazole because of -
            A. Greater efficacy
            B. Longer t1/2
            C. Lesser side effects
            D. All the above
            Answer: D
            
            Question: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction
            A. ab
            B. a
            C. ad
            D. bc
            Answer: A
            
            Question: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -
            A. Additional intake of 300 K.cal
            B. Additional intake of 500 K.cal
            C. Additional intake of 650 K.cal
            D. None
            Answer: D
            
            Question: Highest level of evidence is seen in:
            A. Case control studies
            B. Meta-analysis
            C. Cohort studies
            D. Systematic review
            Answer:
          } [0.0s]
        } [0.0s]
      } [22.932s]
      1000 requests
    } [22.958s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.437s]
        } [2.437s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.205s]
        } [2.205s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.169s]
        } [2.169s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.315s]
        } [2.315s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.237s]
        } [2.237s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.182s]
        } [2.182s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.217s]
        } [2.217s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.249s]
        } [2.249s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.22s]
        } [2.22s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.173s]
        } [2.173s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.184s]
        } [2.184s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.184s]
        } [2.184s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.214s]
        } [2.214s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.201s]
        } [2.201s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.221s]
        } [2.221s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.172s]
        } [2.172s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.228s]
        } [2.228s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.208s]
        } [2.208s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.218s]
        } [2.218s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.172s]
        } [2.173s]
        
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.19s]
        } [2.19s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.196s]
        } [2.196s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.195s]
        } [2.195s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.191s]
        } [2.191s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.202s]
        } [2.202s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m24.762s]
    } [5m24.762s]
  } [5m54.419s]
  Error when running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-8bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', embedding=False, prompt='Give a letter answer among A, B, C or D.\n\nQuestion: Increased perinatal moality in diabetic pregnancy is due to:\nA. Congenital malformations\nB. Hypoglycemia\nC. Hyaline membrane disease\nD. All of the above\nAnswer: D\n\nQuestion: Necrotic Keratinocyts occur in\nA. DLE\nB. Graft versus host disease\nC. Erythema multiformal\nD. All\nAnswer: D\n\nQuestion: For systemic mycosis fluconazole is preferred over ketoconazole because of -\nA. Greater efficacy\nB. Longer t1/2\nC. Lesser side effects\nD. All the above\nAnswer: D\n\nQuestion: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction\nA. ab\nB. a\nC. ad\nD. bc\nAnswer: A\n\nQuestion: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -\nA. Additional intake of 300 K.cal\nB. Additional intake of 500 K.cal\nC. Additional intake of 650 K.cal\nD. None\nAnswer: D\n\nQuestion: Highest level of evidence is seen in:\nA. Case control studies\nB. Meta-analysis\nC. Cohort studies\nD. Systematic review\nAnswer:', temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=15VkJdq5eyWIkfb_aoD3oS8i4tScbHYky&confirm=t because benchmark_output/scenarios/med_mcqa/data already exists
      } [0.096s]
    } [5.089s]
    187005 instances, 182822 train instances, 1000/4183 eval instances
    DataPreprocessor.preprocess {
    } [0.027s]
    MultipleChoiceJointAdapter.adapt {
      183822 instances, choosing 5/182822 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [14.241s]
        Sample prompts {
          reference index = None, request_mode = None {
            Give a letter answer among A, B, C or D.
            
            Question: Increased perinatal moality in diabetic pregnancy is due to:
            A. Congenital malformations
            B. Hypoglycemia
            C. Hyaline membrane disease
            D. All of the above
            Answer: D
            
            Question: Necrotic Keratinocyts occur in
            A. DLE
            B. Graft versus host disease
            C. Erythema multiformal
            D. All
            Answer: D
            
            Question: For systemic mycosis fluconazole is preferred over ketoconazole because of -
            A. Greater efficacy
            B. Longer t1/2
            C. Lesser side effects
            D. All the above
            Answer: D
            
            Question: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction
            A. ab
            B. a
            C. ad
            D. bc
            Answer: A
            
            Question: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -
            A. Additional intake of 300 K.cal
            B. Additional intake of 500 K.cal
            C. Additional intake of 650 K.cal
            D. None
            Answer: D
            
            Question: Highest level of evidence is seen in:
            A. Case control studies
            B. Meta-analysis
            C. Cohort studies
            D. Systematic review
            Answer:
          } [0.0s]
        } [0.0s]
      } [14.741s]
      1000 requests
    } [14.767s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [8.945s]
        } [8.945s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.854s]
        } [2.854s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.714s]
        } [2.714s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.658s]
        } [2.658s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.559s]
        } [2.559s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.662s]
        } [2.662s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.646s]
        } [2.646s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.616s]
        } [2.616s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.575s]
        } [2.575s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.674s]
        } [2.674s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.676s]
        } [2.676s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.58s]
        } [2.58s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.598s]
        } [2.598s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.674s]
        } [2.674s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.6s]
        } [2.6s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.612s]
        } [2.612s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.567s]
        } [2.567s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.69s]
        } [2.69s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.688s]
        } [2.688s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.674s]
        } [2.674s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.64s]
        } [2.64s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.545s]
        } [2.545s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.612s]
        } [2.613s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.608s]
        } [2.608s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_4bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.539s]
        } [2.539s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

      } [5m32.476s]
    } [5m32.476s]
  } [5m54.212s]
  Error when running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', embedding=False, prompt='Give a letter answer among A, B, C or D.\n\nQuestion: Increased perinatal moality in diabetic pregnancy is due to:\nA. Congenital malformations\nB. Hypoglycemia\nC. Hyaline membrane disease\nD. All of the above\nAnswer: D\n\nQuestion: Necrotic Keratinocyts occur in\nA. DLE\nB. Graft versus host disease\nC. Erythema multiformal\nD. All\nAnswer: D\n\nQuestion: For systemic mycosis fluconazole is preferred over ketoconazole because of -\nA. Greater efficacy\nB. Longer t1/2\nC. Lesser side effects\nD. All the above\nAnswer: D\n\nQuestion: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction\nA. ab\nB. a\nC. ad\nD. bc\nAnswer: A\n\nQuestion: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -\nA. Additional intake of 300 K.cal\nB. Additional intake of 500 K.cal\nC. Additional intake of 650 K.cal\nD. None\nAnswer: D\n\nQuestion: Highest level of evidence is seen in:\nA. Case control studies\nB. Meta-analysis\nC. Cohort studies\nD. Systematic review\nAnswer:', temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

  Running med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=15VkJdq5eyWIkfb_aoD3oS8i4tScbHYky&confirm=t because benchmark_output/scenarios/med_mcqa/data already exists
      } [0.077s]
    } [5.091s]
    187005 instances, 182822 train instances, 1000/4183 eval instances
    DataPreprocessor.preprocess {
    } [0.027s]
    MultipleChoiceJointAdapter.adapt {
      183822 instances, choosing 5/182822 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
        } [7.3s]
        Sample prompts {
          reference index = None, request_mode = None {
            Give a letter answer among A, B, C or D.
            
            Question: Increased perinatal moality in diabetic pregnancy is due to:
            A. Congenital malformations
            B. Hypoglycemia
            C. Hyaline membrane disease
            D. All of the above
            Answer: D
            
            Question: Necrotic Keratinocyts occur in
            A. DLE
            B. Graft versus host disease
            C. Erythema multiformal
            D. All
            Answer: D
            
            Question: For systemic mycosis fluconazole is preferred over ketoconazole because of -
            A. Greater efficacy
            B. Longer t1/2
            C. Lesser side effects
            D. All the above
            Answer: D
            
            Question: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction
            A. ab
            B. a
            C. ad
            D. bc
            Answer: A
            
            Question: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -
            A. Additional intake of 300 K.cal
            B. Additional intake of 500 K.cal
            C. Additional intake of 650 K.cal
            D. None
            Answer: D
            
            Question: Highest level of evidence is seen in:
            A. Case control studies
            B. Meta-analysis
            C. Cohort studies
            D. Systematic review
            Answer:
          } [0.0s]
        } [0.0s]
      } [7.814s]
      1000 requests
    } [7.841s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.68s]
        } [2.68s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.624s]
        } [2.624s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.609s]
        } [2.609s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.599s]
        } [2.599s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.582s]
        } [2.582s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.585s]
        } [2.585s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.583s]
        } [2.583s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.604s]
        } [2.604s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.569s]
        } [2.569s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.578s]
        } [2.578s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.613s]
        } [2.614s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.63s]
        } [2.63s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.626s]
        } [2.626s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.58s]
        } [2.58s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.557s]
        } [2.557s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.615s]
        } [2.615s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.585s]
        } [2.585s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.613s]
        } [2.613s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

            Request failed. Retrying (attempt #2) in 10 seconds... (See above for error details)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.605s]
        } [2.605s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.593s]
        } [2.593s]
        
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
            Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.638s]
        } [2.639s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.593s]
        } [2.593s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #3) in 20 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.609s]
        } [2.609s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #4) in 40 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
You shouldn't move a model that is dispatched using accelerate hooks.
          } [2.62s]
        } [2.62s]
        
        Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

        Request failed. Retrying (attempt #5) in 80 seconds... (See above for error details)
        Loading /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 (kwargs={'load_in_8bit': True}) for HELM model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit with Hugging Face Transformers {
          Hugging Face device set to "cuda:0" because CUDA is available.
          Loading Hugging Face model /share/pi/nigam/users/migufuen/temp/pruned_llama/models/Llama-3.2-1B-Instruct_pruned_0.1 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
          } [2.566s]
        } [2.566s]
      } [5m26.279s]
    } [5m26.279s]
You shouldn't move a model that is dispatched using accelerate hooks.
  } [5m40.949s]
} [48m28.66s]

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

Error in sys.excepthook:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/linecache.py", line 72, in checkcache
    stat = os.stat(fullname)
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 114, in process
    result: RequestResult = self.service.make_request(self.execution_spec.auth, state.request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/services/server_service.py", line 146, in make_request
    request_result: RequestResult = self.client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 121, in make_request
    return make_request_with_retry(client=client, request=request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/proxy/retry.py", line 72, in <lambda>
    return lambda f: lambda *args, **kwargs: _retrying.call(f, *args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 266, in call
    raise attempt.get()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/six.py", line 719, in reraise
    raise value
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/retrying.py", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/auto_client.py", line 116, in make_request_with_retry
    return client.make_request(request)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 301, in make_request
    huggingface_model: HuggingFaceServer = HuggingFaceServerFactory.get_server(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 229, in get_server
    HuggingFaceServerFactory._servers[helm_model_name] = HuggingFaceServer(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/clients/huggingface_client.py", line 106, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, **kwargs).to(
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/accelerate/big_modeling.py", line 457, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2883, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 116, in process
    raise ExecutorError(f"{str(e)} Request: {state.request}") from e
helm.benchmark.executor.ExecutorError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`. Request: Request(model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', embedding=False, prompt='Give a letter answer among A, B, C or D.\n\nQuestion: Increased perinatal moality in diabetic pregnancy is due to:\nA. Congenital malformations\nB. Hypoglycemia\nC. Hyaline membrane disease\nD. All of the above\nAnswer: D\n\nQuestion: Necrotic Keratinocyts occur in\nA. DLE\nB. Graft versus host disease\nC. Erythema multiformal\nD. All\nAnswer: D\n\nQuestion: For systemic mycosis fluconazole is preferred over ketoconazole because of -\nA. Greater efficacy\nB. Longer t1/2\nC. Lesser side effects\nD. All the above\nAnswer: D\n\nQuestion: Pulmonary hypertension in COPD is due toa) Constriction of pulm vesselsb) Hypoxiac) Interstitial fibrosisd) Bronchoconstriction\nA. ab\nB. a\nC. ad\nD. bc\nAnswer: A\n\nQuestion: A 24 years old primigravida wt = 57 kg, Hb 11,0 gm% visits an antenatal clinic during 2nd trimester of pregnancy seeking advice on dietary intake. She should be advised -\nA. Additional intake of 300 K.cal\nB. Additional intake of 500 K.cal\nC. Additional intake of 650 K.cal\nD. None\nAnswer: D\n\nQuestion: Highest level of evidence is seen in:\nA. Case control studies\nB. Meta-analysis\nC. Cohort studies\nD. Systematic review\nAnswer:', temperature=0.0, num_completions=1, top_k_per_token=5, max_tokens=1, stop_sequences=[], echo_prompt=False, top_p=1, presence_penalty=0, frequency_penalty=0, random=None, messages=None, multimodal_prompt=None, image_generation_parameters=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/bin/helm-run", line 8, in <module>
    sys.exit(main())
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/run.py", line 350, in main
    run_benchmarking(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/run.py", line 127, in run_benchmarking
    runner.run_all(run_specs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 234, in parallel_map
    with ThreadPoolExecutor(max_workers=parallelism) as executor:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 649, in __exit__
    self.shutdown(wait=True)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 235, in shutdown
    t.join()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
