main {
  Reading tokenizer configs from /share/pi/nigam/users/migufuen/helm/src/helm/config/tokenizer_configs.yaml...
  Reading model deployments from /share/pi/nigam/users/migufuen/helm/src/helm/config/model_deployments.yaml...
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct of model deployment migufuen/Llama-3.2-1B-Instruct
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.1 of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.1
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.2 of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.2
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.3 of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.3
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-4bit of model deployment migufuen/Llama-3.2-1B-Instruct-4bit
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-8bit of model deployment migufuen/Llama-3.2-1B-Instruct-8bit
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit
  WARNING: Could not find model metadata for model migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit of model deployment migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit
  Read 28 run entries from 5_evaluate_pruned_quantized_models.conf
  28 entries produced 28 run specs
  run_specs {
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_qa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='med_mcqa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.med_mcqa_scenario.MedMCQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Give a letter answer among A, B, C or D.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['med_mcqa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='pubmed_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.pubmed_qa_scenario.PubMedQAScenario', args={}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Answer A for yes, B for no or C for maybe.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['pubmed_qa'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'], annotators=None)
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='live_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.live_qa_scenario.LiveQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'f1_score', 'rouge_l', 'bleu_1', 'bleu_4']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['live_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.live_qa_annotator.LiveQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='medication_qa:model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.medication_qa_scenario.MedicationQAScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Please answer the following consumer health question.\n', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=0, max_eval_instances=1000, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=512, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.medication_qa_metrics.MedicationQAScoreMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['medication_qa'], annotators=[AnnotatorSpec(class_name='helm.benchmark.annotation.medication_qa_annotator.MedicationQAAnnotator', args={})])
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-4bit', model='migufuen/Llama-3.2-1B-Instruct-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-8bit', model='migufuen/Llama-3.2-1B-Instruct-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-4bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-4bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=migufuen_Llama-3.2-1B-Instruct-pruned-0.1-8bit', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\n', reference_prefix='A. ', reference_suffix='\n', output_prefix='Answer: ', output_suffix='\n', instance_prefix='\n', substitutions=[], max_train_instances=5, max_eval_instances=1000, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', model='migufuen/Llama-3.2-1B-Instruct-pruned-0.1-8bit', temperature=0.0, max_tokens=1, stop_sequences=['\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)
  } [0.0s]
  Running in local mode with base path: prod_env
Looking in path: prod_env
  AutoTokenizer: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  AutoClient: file_storage_path = prod_env/cache
  AutoClient: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  AutoTokenizer: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  Found 1 account(s).
Looking in path: prod_env
  AnnotatorFactory: file_storage_path = prod_env/cache
  AnnotatorFactory: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')
  Running med_qa:model=migufuen_Llama-3.2-1B-Instruct-4bit {
    scenario.get_instances {
      ensure_file_downloaded {
        Not downloading https://drive.google.com/uc?export=download&id=1ImYUSLk9JbgHXOemfvyiDiirluZHPeQw&confirm=t because benchmark_output/scenarios/med_qa/data already exists
      } [7.234s]
    } [7.825s]
    12723 instances, 10178 train instances, 1000/2545 eval instances
    DataPreprocessor.preprocess {
    } [0.001s]
    MultipleChoiceJointAdapter.adapt {
      11178 instances, choosing 5/10178 train instances, 1000 eval instances
      Adapting with train_trial_index=0 {
        Sampled 5 examples for trial #0.
        Parallelizing computation on 1000 items over 4 threads {
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
          Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        } [1m57.158s]
        Sample prompts {
          reference index = None, request_mode = None {
            The following are multiple choice questions (with answers) about medicine.
            
            Question: A 17-year-old girl comes to the physician because of left lower abdominal pain for 1 day. She describes the pain as 6 out of 10 in intensity. Over the past 5 months, she has had similar episodes of pain that occur once a month and last 1 to 2 days. Menses occur at regular 28-day intervals and last 5 to 6 days. Menarche was at the age of 13 years, and her last menstrual period was 2 weeks ago. She has been sexually active with 1 male partner in the past and has used condoms inconsistently. She tested negative for sexually transmitted infections on her last visit 6 months ago. Abdominal and pelvic examination shows no abnormalities. A urine pregnancy test is negative. Which of the following is the most appropriate next step in the management of this patient's symptoms?
            A. CT scan of the pelvis
            B. Reassurance
            C. Combined oral contraceptive pill
            D. Pelvic ultrasonography
            Answer: B
            
            Question: A 47-year-old man presents to the clinic with a 10-day history of a sore throat and fever. He has a past medical history significant for ulcerative colitis and chronic lower back pain. He smokes at least 1 pack of cigarettes daily for 10 years. The father of the patient died of colon cancer at the age of 50. He takes sulfasalazine and naproxen. The temperature is 38.9C (102.0F), the blood pressure is 131/87 mm Hg, the pulse is 74/min, and the respiratory rate is 16/min. On physical examination, the patient appears tired and ill. His pharynx is erythematous with exudate along the tonsillar crypts. The strep test comes back positive. In addition to treating the bacterial infection, what else would you recommend for the patient at this time?
            A. Fecal occult blood testing
            B. Flexible sigmoidoscopy
            C. Low-dose CT
            D. Colonoscopy
            Answer: D
            
            Question: Blood cultures are sent to the laboratory. Intravenous antibiotic therapy is started. Transesophageal echocardiography shows a large, oscillating vegetation attached to the tricuspid valve. There are multiple small vegetations attached to tips of the tricuspid valve leaflets. There is moderate tricuspid regurgitation. The left side of the heart and the ejection fraction are normal. Which of the following is the most likely causal organism of this patient's conditions?
            A. Streptococcus sanguinis
            B. Enterococcus faecalis
            C. Neisseria gonorrhoeae
            D. Staphylococcus aureus
            Answer: D
            
            Question: A 45-year-old man comes to the physician because of a 3-month history of recurrent headaches. The headaches are of a dull, nonpulsating quality. The patient denies nausea, vomiting, photophobia, or phonophobia. Neurologic examination shows no abnormalities. The physician prescribes a drug that irreversibly inhibits cyclooxygenase-1 and cyclooxygenase-2 by covalent acetylation. Which of the following medications was most likely prescribed by the physician?
            A. Indomethacin
            B. Aspirin
            C. Celecoxib
            D. Carbamazepine
            Answer: B
            
            Question: A 58-year-old male with a history of congestive heart failure and hypertension comes to you with the chief complaint of new-onset cough as well as increased serum potassium in the setting of a new medication. Which of the following medications is most likely responsible for these findings?
            A. Furosemide
            B. Amiodarone
            C. Digoxin
            D. Lisinopril
            Answer: D
            
            Question: A 47-year-old man comes to the physician because of severe retrosternal chest pain and shortness of breath for 45 minutes. He has dyslipidemia, hypertension, and type 2 diabetes mellitus. Current medications include hydrochlorothiazide, lisinopril, metformin, and atorvastatin. He has smoked 1 pack of cigarettes daily for 20 years. He appears pale and diaphoretic. His temperature is 37C (98.6F), pulse is 115/min, and blood pressure is 140/70 mm Hg. Breath sounds are normal. The remainder of the examination shows no abnormalities. An ECG shows left ventricular hypertrophy with ST-segment elevation in leads I, aVL, and V1V6. High-dose aspirin, clopidogrel, metoprolol, sublingual nitroglycerin, and unfractionated heparin are administered. As the patient awaits transport to the nearest emergency room, he collapses and becomes unresponsive. His pulse and blood pressure cannot be detected. Despite resuscitative efforts, the patient dies. Which of the following is the most likely cause of death in this patient?
            A. Papillary muscle rupture
            B. Ventricular fibrillation
            C. Septal wall rupture
            D. Pulmonary embolism
            Answer:
          } [0.0s]
        } [0.0s]
      } [1m57.184s]
      1000 requests
    } [1m57.186s]
    Executor.execute {
      Parallelizing computation on 1000 items over 4 threads {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/meta.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        Loading meta-llama/Meta-Llama-3-8B (kwargs={}) for HELM tokenizer meta/llama-3-8b with Hugging Face Transformers {
        Created cache with config: SqliteCacheConfig(path='prod_env/cache/migufuen.sqlite')
        } [0.778s]
        Loading /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 (kwargs={'load_in_4bit': True, 'device_map': 'auto'}) for HELM model migufuen/Llama-3.2-1B-Instruct-4bit with Hugging Face Transformers {
          Hugging Face device_map set to "auto".
          Loading Hugging Face model /share/pi/nigam/users/migufuen/.cache/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6 {
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
          } [1m12.626s]
        } [1m20.417s]
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(
2024-11-10 03:27:22.515020: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-10 03:27:23.059193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-10 03:27:23.355199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-10 03:27:23.436372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-10 03:27:24.277615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
      } [2m39.099s]
    } [2m39.108s]
  } [4m44.336s]
} [5m49.878s]
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 235, in parallel_map
    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 621, in result_iterator
    yield _result_or_cancel(fs.pop())
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 319, in _result_or_cancel
    return fut.result(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 453, in result
    self._condition.wait(timeout)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 320, in wait
    waiter.acquire()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/bin/helm-run", line 8, in <module>
    sys.exit(main())
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/run.py", line 350, in main
    run_benchmarking(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/run.py", line 127, in run_benchmarking
    runner.run_all(run_specs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 216, in run_all
    self.run_one(run_spec)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/runner.py", line 291, in run_one
    scenario_state = self.executor.execute(scenario_state)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/hierarchical_logger.py", line 104, in wrapper
    return fn(*args, **kwargs)
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/executor.py", line 99, in execute
    request_states = parallel_map(
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/general.py", line 234, in parallel_map
    with ThreadPoolExecutor(max_workers=parallelism) as executor:
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/_base.py", line 649, in __exit__
    self.shutdown(wait=True)
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 235, in shutdown
    t.join()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
Exception ignored in: <module 'threading' from '/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py'>
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1537, in _shutdown
    atexit_call()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/concurrent/futures/thread.py", line 31, in _python_exit
    t.join()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt: 
Traceback (most recent call last):
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/bin/helm-summarize", line 5, in <module>
    from helm.benchmark.presentation.summarize import main
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/presentation/summarize.py", line 34, in <module>
    from helm.common.codec import from_json
  File "/share/pi/nigam/users/migufuen/helm/src/helm/common/codec.py", line 21, in <module>
    from helm.benchmark.augmentations.synonym_perturbation import SynonymPerturbation
  File "/share/pi/nigam/users/migufuen/helm/src/helm/benchmark/augmentations/synonym_perturbation.py", line 10, in <module>
    import spacy
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/spacy/__init__.py", line 6, in <module>
    from .errors import setup_default_warnings
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/spacy/errors.py", line 3, in <module>
    from .compat import Literal
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/spacy/compat.py", line 39, in <module>
    from thinc.api import Optimizer  # noqa: F401
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/thinc/api.py", line 23, in <module>
    from .layers import (
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/thinc/layers/__init__.py", line 3, in <module>
    from .add import add
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/thinc/layers/add.py", line 4, in <module>
    from ..model import Model
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/thinc/model.py", line 28, in <module>
    from .shims import Shim
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/thinc/shims/__init__.py", line 1, in <module>
    from .mxnet import MXNetShim
  File "/share/pi/nigam/users/migufuen/conda/envs/crfm-helm/lib/python3.10/site-packages/thinc/shims/mxnet.py", line 17, in <module>
    from .shim import Shim
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
KeyboardInterrupt
